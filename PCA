# 人眼只能看到三维的数据，因此三维以上需要降维（降低特征矩阵中特征的数量）
# 降维流程：
# 1）找出原本的n个特征向量构成的n维空间；
# 2）决定降维后的特征数量；
# 3）通过某种变化，找出n个新的特征向量，以及它们 构成的新n维空间；
# 4）找出原始数据在新特征空间V中的n个新特征向量上 对应的值，即“将数据映射到新空间中”；
# 5）选取前k个信息量大的特征，删掉没有被选中的 特征，成功将n维空间V降为k维
# 矩阵分解：找出n个新特征向量，让数据能够被压缩到少数特征上并且总信息量不损失太多

import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA
import pandas as pd
import numpy as np

iris = load_iris()  #导入鸢尾花数据
data = iris.data
target = iris.target   #返回0,1,2三种数据，有三种鸢尾花
print(data.shape)  #结果：(150, 4)   作为数组data是二维的,作为特征矩阵是四维的

######################### 调用PCA降维 ##############################
pca = PCA(n_components=2)
result = pca.fit_transform(data)   #降维后的新特征矩阵
print(result)  #将为后的数据只有两列，是二维的
print(result[target == 0,0])   #通过布尔索引返回标签值为0的鸢尾花对应的新特征矩阵中的第0列，即第一种花的第一个特征
print(result[target == 0,1])   #第一种花的第二个特征

################### 画图 ########################
a0 = result[target == 0,0]
a1 = result[target == 0,1]
b0 = result[target == 1,0]
b1 = result[target == 1,1]
c0 = result[target == 2,0]
c1 = result[target == 2,1]
# 三种散点图分别对应三种花
plt.figure()
plt.scatter(a0,a1,c="r",label=iris.target_names[0])
plt.scatter(b0,b1,c="b",label=iris.target_names[1])
plt.scatter(c0,c1,c="g",label=iris.target_names[2])
plt.legend()
plt.title('PCA of IRIS dataset')
plt.show()

# 用for循环画图，效果和上面一样
colors = ["r","b","g"]   #先定义一个颜色数组
plt.figure()
for i in [0,1,2]:
    plt.scatter(result[target == i,0],result[target == i,1],
                c=colors[i],label=iris.target_names[i])
plt.legend()
plt.title('PCA of IRIS dataset')
plt.show()

############################ 探索降维后的数据 ############################
var_new = pca.explained_variance_  # 属性explained_variance，查看降维后每个新特征向量上所带的信息量大小（可解释性方差的大小）
print(var_new)   #结果：[4.22824171 0.24267075]
var_ratio = pca.explained_variance_ratio_   #属性explained_variance_ratio，查看降维后每个新特征向量所占的信息量占原始数据总信息量的百分比(可解释方差贡献率)
print(var_ratio)    #结果：[0.92461872 0.05306648]
# 可以看到信息量及其占比都是第一个特征更大，说明降维后大部分信息都被有效地集中在了第一个特征上
var_ratio_sum = pca.explained_variance_ratio_.sum()    #对可解释性方差贡献率进行加和查看新特征矩阵所含信息量占原特征矩阵的比例
print(var_ratio_sum)   #结果：0.977685206318795  说明砍掉部分数据后仍然保留了绝大部分信息，将为效果比较好

###################确定n_components最佳取值################################
# 累积可解释方差贡献率曲线：以降维后保留的特征个数为横坐标，降维后新特征矩阵捕捉到的可解释方差贡献率为纵坐标
pca_line = PCA().fit(data)
result_ratio = pca_line.explained_variance_ratio_   #会返回每一个特征向量那一列所带的信息量占原始数据信息量的比值
print(result_ratio)   #结果：[0.92461872 0.05306648 0.01710261 0.00521218]
plt.plot([1,2,3,4],np.cumsum(result_ratio))
plt.xticks([1,2,3,4])
plt.xlabel("number of components after dimension reduction")  #X轴：降维后特征数量
plt.ylabel("cumulative explained variance")    #Y轴：累计累积可解释方差
plt.show()
# 选择的时候选择让曲线突然变得平滑的点

# 最大似然估计自选超参数——输入mle直接调用
pca_mle = PCA(n_components="mle")
data_mle = pca_mle.fit_transform(data)
print(data_mle)  # 结果有三列，mle为自动选择了3个特征

# 按信息量占比选超参数
pca_f = PCA(n_components=0.97,svd_solver="full")  #输入[0,1]之间的浮点数n并且让参数svd_solver =='full',PCA会自动选出能够让保留的信息量超过n的特征数量
data_f = pca_f.fit_transform(data)
print(data_f)
print(pca_f.explained_variance_ratio_)





